dock# General Knowledge
A running instance of an image is called *container*. 

Those are two different things, a **Docker Image** is a combination of filesystem and parameters, you can see like a package able to run anything you need or a factory to build containers. An image it does not have **any state** and once build it **never changes**, it is something that you can download, build and run.

While we are running an image we create a **Docker Container**, it's an instance of the image that we're working on. We can lunch many container from a single image, like we do at OddCamp with our Rails template. The image is always the same but we create many containers as many applications we create. A container is also immutable, meaning that if we apply some changes and stop it, all the changes will get lost. There are workarounds to this connecting the local folders in our container with the ones that are in the host system.

We can run any image hosted on the Docker Hub with `docker run <image_name>`

By default a running Docker Container (`docker run`) will only forward the container's output to the client. If we want to send some commands, via bash for. example, we nne to be explixit with Docker and tell it to forward the CLI input from the Docker deamon (that on Macs and Windows machine lives inside a Linux VM) and we need to add the `-i` option to the `run` command.

But since an interactive Bash sessions needs to be run inside a *termal emulator* (reponsible to display a prompt and use *Ctrl+C*) and we can do this by addint the `-t` option. 

You can easily remember this in a single option `-it` and imagine that the meaning of it is *`-i`n`-t`eractive*.

# Building images
Sometime is useful to have our own Images that we can use to create custom Containers. If you create a `Dockerfile` you have to generate the Image or, better yet, **build** it.

We can build an Image with `docker build [options] /path/to/build/dir`
# Volumes
A volume let's you to clone a directory that lives inside your container with one that is available on your system, so you'll be able to apply changes and save them for later use.

> **Remember:** if you do not mount a volume all the edits you make inside a container will be lost when the container will shut down.

Without a volume in place each time we make an edit we have to build the image for the container, this is because while building Docker will copy the changed files into the new container.
> **Reminder:** To build an image you need to run `docker image build <image_name> <container_directory>`, always build an image when you edit the *Dockerfile*

One of the way we can add a volume while running a container is to set the `-v` option to the `run` command.
# Debugging Tips and Tricks
We can connect to an already running Docker container to execute in it some commands in order to check if everything is good.

In the course the first issue is to handle an error that the Alpine image can cause on Mac or Windows. The solution is to build the image with a different operating system that has more feature, so we changed in the *Dockerfile* the image we use in `FROM` with `python:3.7.5-slim` and build the image back with `docker image build -t <container_name> .`

Beside this edge case most of the time you want to open a terminal into the container to check what is going on, in order to do so you have to use the `exec` command of `docker container`.

For example, if you use Alpine, you can open a shell with `docker container -it <container_name> sh`. In the course we need to do this to delete the cache files generated by Python.

# Link container with a network
By default Docker creates some network for us and we can check them out with `docker network ls` and within the many networks that you can have installed you'll see one named `bridge`.

We can inspect the network with `docker network instpect <network_name>`.

Since `exec` is able to run commands for us we can run the Linux command `ifconfig` to know the local address of a container, just type `docker exec <container_name> ifconfig`, but you find the same information inside the `network inspect command`,

To test that the two containers can communicate between each other we can just send a ping one container from the other. Assuming that the one of your container has the IP of `172.17.0.2` you can ping it from the container named `web2` like so: `docker exec web2 ping 172.17.0.2`, if the server respond you're in business.

This setup has it's own drawbacks, for example each time we run a container Docker will set the IP addresses randomly and this is the problem with the default bridge network because if we want to use the Redis container from our Flask one we should manually set the IPs each time.

We can set a custom bridge network so Docker will configure the DNS automatically and we can send request using the container name.

To create a custom bridge network all we need to do is to pass the `create` command to  `docker network` like so: `docker network create --driver bridge firstnetwork`

Now that we have a network we need to connect the containers to it, you can see with `docker network inspect firstnetwork` that no containers are added.

To connect a container to a specific network we need to add the `--net <network_name>` option when we `run` the container. Once the containers are connected to the same network you can `ping` (for example) each other using their names.

`docker exec web2 ping redis`

This command will send a `ping` from the container `web2` to the container `redis`.

# Persisting data in the Docker Host
Since containers are immutable, when we stop one we lost all the information. This is has happen also to the Redis container we were experimenting with in the previous lesson, if we want to persist the data we can use data volumes. And this allow us to restore the data even when we shut down the container.

Instead of giving a path to the volume, as we did with `$PWD:/app`, this time we're going to use a **named volume** with this kind of syntax `web2_redis:/data` this allow us to provide just a name and Docker will manage the volume for us in a special `/data` directory.

We can create a volume with the special `volume` command that Docker provide us. So if you want to create a named volume `web2_redis` you should type `docker volume create web2_redis`

You can check it's presence with `docker volume ls` and also get more information about it with `docker volume inspect web2_redis`

Once you've created your volume you can connect your container to it with the `-v` option specifying the name of the volume and following the convention to where to store the data stored in the volume. In case of a Redis container you can connect the volume to the `/data` directory with `-v web2_redis:/data`

# Share data between containers
In order to share data between containers we first need to expose some folders from one of our containers. For example in the lesson we share the `/app/public` folder by adding the following line to our *Dockerfile*
`VOLUME ["/app/public"]`

Now this folder is accessible to other containers by adding the following option: `--volumes-from web2` by adding this to our `docker container run` command we say that we want to use the volumes that the container `web2` exposes.

If we want to test that everything is working, assuming that you have used the previous option running the `redis` container in the previous examples, we need to enter in the console of that container (you can do so with `docker container exec -it redis sh`, assuming the container is named `redis`) and once in we can just `cd /app/public` and list the files in here.  If the list contains a `main.css` file you're in business.

Now that the volumes are connected, if you change something, the changes will be reflected in each container. Even if the changes you make in the folder will be shared between containers those will not be reflected in your host, that's because even if the folder we share (`/public`) is inside a folder we have mounted in a volume (`/app`) Docker will not reflect those changes.

As before though, if you change something in `/app` your changes will be reflected in the host system too and in both containers as well.
# Optimizing Docker Images
You can start to ignore by patterns with *.dockerignore* file. If we have this file while building the image during the **COPY/ADD** instructions it removes the files and folder we specified in it, if **WORKDIR** is set then that folder will be the starting point of all our ignore paths.

Inside the *.dockerignore* file you can:
* `.dockerignore` - set specific files to be ignored
* `.git/` - set specific folders to be ignored, `/` is optional but clearly show that is a folder
* `.git/*` - ignore all contents in a folder but allow the folder to exist
* `**/*.scss` - ignore all the files (everywhere) with a specific extension
* `!main.scss` - negate a previous rule for a specific file, in this case even if before we set to ignore all `.scss` files we allow the one named `main.scss` to be used in the build image

Another approach to optimize the image we could run some scripts that let you remove the build dependency and in some case you can save up to 250% of disk space but this is an approach that should be taken only when your image causes issues. Most of the time the images are already optimized and remember that Docker is very efficient by itself so don't waste nights on optimizations if you don't have to.
#  Running Scripts when a Container Starts
This is useful if we want to use a single image for multiple projects that have slight different configurations.

For example if you want to use a different PostgreSQL database for each application, and you can do adding `ENTRYPOINT` to your *Dockerfile*. So you can link to a shell script that is connected.

The first thing we need to do to make this work is to `COPY` our script into the folder that will be mounted in our container. Since in the example we have the script `docker-entrypoint.sh` we insert this in our *Dockerfile*:
```
COPY docker-entrypoint.sh /
```
As we should know by now, this will copy our file in the root directory of the running container. Be aware that the script is in the folder of the image.

Next we need to make sure that the script is executable, so we `RUN` a `chmod` for this while the container is starting:
```
RUN chmod +x /docker-entrypoint.sh
```
At the end the `ENTRYPOINT` instruction is added to let the container run it.
```
ENTRYPOINT ["/docker-entrypoint.sh"]
```
In the lesson the example that's shown regards a text string that we would like to change **without rebuilding the image**.

The script is pretty simple, has a bunch of settings to make our script work, accept an env variable named `WEB2_COUNTER_MSG` with a default value and then has a bash magic `exec "$@"`.

The `CMD` instruction passes everything we supply to it over to an `ENTRYPOINT` instruction, even if you don't define it. By default the `ENTRYPOINT` defined by Docker is `/bin/sh -c`. This is our `CMD` instruction in the *Dockerfile*: `CMD ["flask", "run", "--host=0.0.0.0", "--port=5000"]` and this is the logic behind it:
![[cmd-instruction-docker.png]]

Now that we defined our own `ENTRYPOINT` the `CMD` instruction gets appendend to our script so `exec "$@"` says that after we run the script it takes every custom argument passed and execute it as a single command.

With `ENTRYPOINT` we can easily run migration after the container starts, or change nginx config.
# Clean after yourself
If you want to see at any time how much space Docker takes in your disc you can use `docker system df`, beside the disc space with this you can have an overview of container, images and local volumes that are present in your system.

For example, if you want to find out how many images are not used at all (dangling images), you can run `docker image ls` and where you see `<none>` it means that we can safely remove this image.

If you have some dangling images or volumes the space they use will be listed in the `RECLAIMABLE` column that appear with the `docker system df` command:
![[docker-system-df-reclaimable-disc-space.png]]

